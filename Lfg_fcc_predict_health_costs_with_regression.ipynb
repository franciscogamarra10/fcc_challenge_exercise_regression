{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de fcc_predict_health_costs_with_regression.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9TX15KOkPBV"
      },
      "source": [
        "*Note: You are currently reading this using Google Colaboratory which is a cloud-hosted version of Jupyter Notebook. This is a document containing both text cells for documentation and runnable code cells. If you are unfamiliar with Jupyter Notebook, watch this 3-minute introduction before starting this challenge: https://www.youtube.com/watch?v=inN8seMm7UI*\n",
        "\n",
        "---\n",
        "\n",
        "In this challenge, you will predict healthcare costs using a regression algorithm.\n",
        "\n",
        "You are given a dataset that contains information about different people including their healthcare costs. Use the data to predict healthcare costs based on new data.\n",
        "\n",
        "The first two cells of this notebook import libraries and the data.\n",
        "\n",
        "Make sure to convert categorical data to numbers. Use 80% of the data as the `train_dataset` and 20% of the data as the `test_dataset`.\n",
        "\n",
        "`pop` off the \"expenses\" column from these datasets to create new datasets called `train_labels` and `test_labels`. Use these labels when training your model.\n",
        "\n",
        "Create a model and train it with the `train_dataset`. Run the final cell in this notebook to check your model. The final cell will use the unseen `test_dataset` to check how well the model generalizes.\n",
        "\n",
        "To pass the challenge, `model.evaluate` must return a Mean Absolute Error of under 3500. This means it predicts health care costs correctly within $3500.\n",
        "\n",
        "The final cell will also predict expenses using the `test_dataset` and graph the results."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install sweetviz\n",
        "%pip install feature-engine\n",
        "%pip install scikit-optimize\n",
        "%pip install hyperopt"
      ],
      "metadata": {
        "id": "DLtLqy4ZbYGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rRo8oNqZ-Rj"
      },
      "source": [
        "# Import libraries. You may or may not use all of these.\n",
        "!pip install -q git+https://github.com/tensorflow/docs\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import tensorflow_docs as tfdocs\n",
        "import tensorflow_docs.plots\n",
        "import tensorflow_docs.modeling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiX2FI4gZtTt"
      },
      "source": [
        "# Import data\n",
        "!wget https://cdn.freecodecamp.org/project-data/health-costs/insurance.csv\n",
        "dataset = pd.read_csv('insurance.csv')\n",
        "dataset.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcopvQh3X-kX"
      },
      "source": [
        "data=dataset\n",
        "import sweetviz as sv\n",
        "# reporte=sv.analyze(data)\n",
        "reporte=sv.analyze(data,target_feat='expenses')\n",
        "# reporte.show_html('titanic.html')\n",
        "reporte.show_notebook()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## es muy razonable que haya una correlación entre expensas y edad, básicamente por grupos de edad seguramente va a haber un salto de costos\n",
        "## el tema de smoke también imagino por el riesgo de costos de futuras patalogías relacionadas\n",
        "## bmi tiene correlación\n",
        "## simplifica mucho el análisis que no hay presencias de Missing Values!\n",
        "target='expenses'\n",
        "#data=data.drop('alive',axis=1)\n",
        "\n",
        "cat_vars = [var for var in data.columns if data[var].dtype == 'O' and var not in target]\n",
        "cat_vars"
      ],
      "metadata": {
        "id": "maJTncHSdIw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_vars = [\n",
        "    var for var in data.columns if var not in cat_vars and var != target\n",
        "]\n",
        "\n",
        "# number of numerical variables\n",
        "num_vars\n"
      ],
      "metadata": {
        "id": "xMoxiJNCdI0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  let's male a list of discrete variables seguramente seran categoricas ordinales o nominales\n",
        "discrete_vars = [var for var in num_vars if len(\n",
        "    # data[var].unique()) < 20 and var not in year_vars]\n",
        "    data[var].unique()) < 20 ]\n",
        "discrete_vars"
      ],
      "metadata": {
        "id": "dFL02b7kdI2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_vars = [\n",
        "    var for var in data.columns if var not in cat_vars and var != target  and var not in discrete_vars\n",
        "]\n",
        "num_vars"
      ],
      "metadata": {
        "id": "FTIxCTjPdI5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###target regre veamos la variable\n",
        "# histogran to evaluate target distribution\n",
        "### skewed not normal\n",
        "data[target].hist(bins=50, density=True)\n",
        "plt.ylabel('N')\n",
        "plt.xlabel('Expenses')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MrNTmHcpd7cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###si hay una relacion monotónica con el target pueden codificarse como ordinales las discretas\n",
        "import seaborn as sns\n",
        "for var in discrete_vars:\n",
        "    # make boxplot with Catplot\n",
        "    sns.catplot(x=var, y='expenses', data=data, kind=\"box\", height=4, aspect=1.5)\n",
        "    # add data points to boxplot with stripplot con aplha\n",
        "    sns.stripplot(x=var, y='expenses', data=data, jitter=0.1, alpha=0.3, color='k')\n",
        "    # print(pd.crosstab(data[var], data['survived'], margins=True, normalize='index'))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "RsCrZdN_d7fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make list of continuous variables\n",
        "cont_vars = [\n",
        "    var for var in num_vars if var not in discrete_vars]\n",
        "\n",
        "print('Number of continuous variables: ', len(cont_vars)) "
      ],
      "metadata": {
        "id": "gBxVcCJod7h8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[cont_vars].hist(bins=30, figsize=(15,15))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0JZayE3pd7k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first make a list with the super skewed variables\n",
        "# for later\n",
        "### muy skewed binarizarlas\n",
        "skewed = [\n",
        "\n",
        "]\n",
        "\n",
        "# capture the remaining continuous variables\n",
        "\n",
        "cont_vars = [\n",
        "    'age',\n",
        "    'bmi'\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "At-p3V88flDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "tmp = data.copy()\n",
        "\n",
        "for var in cont_vars:\n",
        "\n",
        "    # transform the variable with logarithm\n",
        "    stats.yeojohnson(data[var])\n",
        "    #tmp[var] = np.log(data[var]+1)\n",
        "    \n",
        "tmp[cont_vars].hist(bins=30)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kJ4uKOVLflIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's plot the original or transformed variables\n",
        "# vs sale price, and see if there is a relationship\n",
        "\n",
        "# for var in [\"LotFrontage\", \"1stFlrSF\", \"GrLivArea\"]:\n",
        "for var in cont_vars:    \n",
        "    plt.figure(figsize=(12,4))\n",
        "    \n",
        "    # plot the original variable vs sale price    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(data[var], np.log(data['expenses']))\n",
        "    plt.ylabel('expenses')\n",
        "    plt.xlabel('Original ' + var)\n",
        "\n",
        "    # plot transformed variable vs sale price\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.scatter(tmp[var], np.log(tmp['expenses']))\n",
        "    plt.ylabel('expenses')\n",
        "    plt.xlabel('Transformed ' + var)\n",
        "                \n",
        "    plt.show()\n",
        "\n",
        "### la transformación no ayuda mucho"
      ],
      "metadata": {
        "id": "CITSbPRnflL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Y0XYmlrtjLzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# las que son skewed extr veamos de binarizarlas\n",
        "## si hay continuas candidatas a discretizarlas aca se ve que cada grupo que se separo\n",
        "## hay una relacion monotonica con la edad\n",
        "from feature_engine.discretisation import EqualFrequencyDiscretiser\n",
        "\n",
        "\n",
        "skewed2=['age','bmi']\n",
        "for var in skewed2:\n",
        "    \n",
        "    disc = EqualFrequencyDiscretiser(q=5, variables = [var])\n",
        "    tmp = data.copy()\n",
        "    disc.fit(tmp)\n",
        "    # map the variable values into 0 and 1\n",
        "    \n",
        "    # tmp[var] = np.where(data[var]==0, 0, 1)\n",
        "    tmp2=disc.transform(tmp)\n",
        "    # determine mean sale price in the mapped values\n",
        "    tmp2 = tmp2.groupby(var)['expenses'].agg(['mean', 'std'])\n",
        "\n",
        "    # plot into a bar graph\n",
        "    tmp2.plot(kind=\"barh\", y=\"mean\", legend=False,\n",
        "             xerr=\"std\", title=\"expenses\", color='green')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "IhDGg7a-flOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# para la ingeniería de variables\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from feature_engine import imputation as mdi\n",
        "from feature_engine import discretisation as dsc\n",
        "from feature_engine import encoding as ce\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop(['expenses'], axis=1),\n",
        "\n",
        "                                                    data['expenses'],\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=123)\n",
        "\n",
        "X_train.shape, X_test.shape\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MYNB6q5hflRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# num_varsf = [var for var in X_train.columns if var in cont_vars or var in skewed or var in year_vars]\n",
        "num_varsf = [var for var in X_train.columns if var in cont_vars or var in skewed ]\n",
        "num_varsf\n",
        "\n",
        "vars_with_na_num = [var for var in num_varsf if X_train[var].isnull().sum() > 0]\n",
        "vars_with_na_num"
      ],
      "metadata": {
        "id": "gSkRQBrPkC42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##transformaremos age y bmi a categorica\n",
        "# from feature_engine.discretisation import EqualFrequencyDiscretiser\n",
        "disc = EqualFrequencyDiscretiser(q=5, variables = ['age','bmi'])\n",
        "\n",
        "disc.fit(X_train)\n",
        "\n",
        "# transformar sets de entrenamiento y prueba\n",
        "\n",
        "X_train = disc.transform(X_train)\n",
        "X_test= disc.transform(X_test)\n"
      ],
      "metadata": {
        "id": "28svaUaTkC7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cat_varsf = [var for var in X_train.columns if X_train[var].dtype == 'O']\n",
        "cat_varsf\n",
        "\n",
        "cat_vars_with_na = [\n",
        "    var for var in cat_varsf\n",
        "    if X_train[var].isnull().sum() > 0\n",
        "]\n",
        "\n",
        "cat_vars_with_na"
      ],
      "metadata": {
        "id": "GqhaEuhSflUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with_string_missing = [\n",
        "    var for var in cat_vars_with_na if X_train[var].isnull().mean() > 0.1]\n",
        "\n",
        "# variables to impute with the most frequent category\n",
        "with_frequent_category = [\n",
        "    var for var in cat_vars_with_na if X_train[var].isnull().mean() < 0.1]\n",
        "\n",
        "with_string_missing\n",
        "\n",
        "\n",
        "with_frequent_category"
      ],
      "metadata": {
        "id": "mL968R7wm_uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_varsf\n",
        "import matplotlib.pyplot as plt\n",
        "N = len(data)\n",
        "\n",
        "# Por cada categoria\n",
        "for col in cat_varsf:\n",
        "\n",
        "    # contar el número de casas por cada categoría\n",
        "    # y dividir por el número total de casas\n",
        "\n",
        "    # esto es, el porcentaje de casas por cada categoría \n",
        "\n",
        "    temp_df = pd.Series(X_train[col].value_counts() / N)\n",
        "\n",
        "    # Creemos una gráfica con los porcentajes anteriores \n",
        "    fig = temp_df.sort_values(ascending=False).plot.bar()\n",
        "    fig.set_xlabel(col)\n",
        "\n",
        "    # Una línea al 5 % marca el límite para las categorias poco comunes\n",
        "    fig.axhline(y=0.05, color='red')\n",
        "    fig.set_ylabel('raras')\n",
        "    plt.show()\n",
        "\n",
        "## no hay categorias raras "
      ],
      "metadata": {
        "id": "ormSHfRynMYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from feature_engine.encoding import OneHotEncoder\n",
        "ohe_enc = OneHotEncoder(\n",
        "    top_categories=5,  # puedes cambiar este valor para seleccionar mas o menos variables\n",
        "    # puedes seleccionar cuales variables codificar\n",
        "    variables=cat_varsf,\n",
        "    drop_last=False)\n",
        "\n",
        "ohe_enc.fit(X_train)\n",
        "\n",
        "X_train = ohe_enc.transform(X_train)\n",
        "X_test = ohe_enc.transform(X_test)"
      ],
      "metadata": {
        "id": "zVyq5u6jnMbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train.shape, X_test.shape\n",
        "\n",
        "df1 = sv.compare(X_train, X_test)\n",
        "# df1.show_html('Compare_train_test')\n",
        "df1.show_notebook()\n",
        "## verifico que haya una similar variabilidad entre train, test"
      ],
      "metadata": {
        "id": "dWa2MloonMd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=X_train\n",
        "train_labels=y_train\n",
        "test_dataset=X_test\n",
        "test_labels=y_test"
      ],
      "metadata": {
        "id": "q_v8041InMgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import lightgbm as lgb\n",
        "\n",
        "\n",
        "np.random.seed(10)\n",
        "\n",
        "\n",
        "# d1 = np.random.randint(2, size=(100, 9))\n",
        "# d2 = np.random.randint(3, size=(100, 9))\n",
        "# d3 = np.random.randint(4, size=(100, 9))\n",
        "# Y = np.random.randint(7, size=(100,))\n",
        "\n",
        "\n",
        "# X = np.column_stack([d1, d2, d3])\n",
        "\n",
        "rs_params = {\n",
        "\n",
        "        'bagging_fraction': (0.5, 0.8,0.9),\n",
        "        'bagging_frequency': (2,5, 8),\n",
        "\n",
        "        'feature_fraction': (0.5, 0.8),\n",
        "        'max_depth': (1,3,10, 20),\n",
        "        'min_data_in_leaf': (30,90, 120),\n",
        "        'num_leaves': (600,1200, 1550)\n",
        "\n",
        "}\n",
        "\n",
        "# Initialize a RandomizedSearchCV object using 5-fold CV-\n",
        "# rs_cv = RandomizedSearchCV(estimator=lgb.LGBMClassifier(), param_distributions=rs_params, cv = 5, n_iter=100,verbose=1)\n",
        "rs_cv = RandomizedSearchCV(estimator=lgb.LGBMRegressor(random_state=10), param_distributions=rs_params, cv = 5, n_iter=100,verbose=1)\n",
        "\n",
        "rs_cv.fit(X_train, y_train,verbose=1)\n"
      ],
      "metadata": {
        "id": "e-foADq-oqPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def build_model():\n",
        "  # model = keras.Sequential([\n",
        "  #   layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),\n",
        "  #   layers.Dense(64, activation='relu'),\n",
        "  #   layers.Dense(1)\n",
        "  # ])\n",
        "\n",
        "  # optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
        "\n",
        "  # model.compile(loss='mse',\n",
        "  #               optimizer=optimizer,\n",
        "  #               metrics=['mae', 'mse'])\n",
        "  num_neuronas = X_train.shape[1] \n",
        "  model= keras.Sequential([\n",
        "    layers.Dense(num_neuronas, activation='relu', input_shape=[len(train_dataset.keys())]),\n",
        "    layers.Dense(num_neuronas, activation='relu'),\n",
        "    layers.Dense(num_neuronas, activation='relu'),\n",
        "    layers.Dense(num_neuronas, activation='relu'),\n",
        "    layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "#Añadir un número de neuronas similar al número de features en X_train - 7 en el ejemplo\n",
        "\n",
        "  # model.add(Dense(num_neuronas,activation='relu'))\n",
        "  # model.add(Dense(num_neuronas,activation='relu'))\n",
        "  # model.add(Dense(num_neuronas,activation='relu'))\n",
        "  # model.add(Dense(num_neuronas,activation='relu'))\n",
        "  # model.add(Dense(1)) # Indicar tantas neuronas de salida como variables a predecir, en este caso solo 1 variable \"buy_price\"\n",
        "\n",
        "  opt = Adam(lr=0.001)\n",
        "  # model.compile(optimizer = opt , loss =tf.keras.losses.BinaryCrossentropy() , metrics = ['accuracy'])\n",
        "  model.compile(optimizer = opt ,loss='mse', metrics=['mae', 'mse'])\n",
        "  # model.compile(optimizer='adam',loss='mse')\n",
        "  return model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K_gp7HRboqTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= build_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "LEjqCN4usmXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "# history = model.fit(train_data_gen,epochs = 30 ,callbacks=[es] ,validation_data = val_data_gen )\n",
        "history =model.fit(x=X_train,y=y_train,\n",
        "          callbacks=[es],\n",
        "          validation_data=(X_test,y_test), #si indicamos validation_data en cada epoch calcula sobre el conjunto de test el resultado de pérdidas\n",
        "          batch_size=128,epochs=500)"
      ],
      "metadata": {
        "id": "F2OvA52drRhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def plot_history(history):\n",
        "#   hist = pd.DataFrame(history.history)\n",
        "#   hist['epoch'] = history.epoch\n",
        "\n",
        "#   plt.figure()\n",
        "#   plt.xlabel('Epoch')\n",
        "#   plt.ylabel('Mean Abs Error [MPG]')\n",
        "#   plt.plot(hist['epoch'], hist['mae'],\n",
        "#            label='Train Error')\n",
        "#   plt.plot(hist['epoch'], hist['val_mae'],\n",
        "#            label = 'Val Error')\n",
        "#   plt.ylim([0,5])\n",
        "#   plt.legend()\n",
        "\n",
        "#   plt.figure()\n",
        "#   plt.xlabel('Epoch')\n",
        "#   plt.ylabel('Mean Square Error [$MPG^2$]')\n",
        "#   plt.plot(hist['epoch'], hist['mse'],\n",
        "#            label='Train Error')\n",
        "#   plt.plot(hist['epoch'], hist['val_mse'],\n",
        "#            label = 'Val Error')\n",
        "#   plt.ylim([0,20])\n",
        "#   plt.legend()\n",
        "#   plt.show()\n",
        "\n",
        "\n",
        "# plot_history(history)"
      ],
      "metadata": {
        "id": "uCr-VmEyuXvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, mae, mse = model.evaluate(test_dataset, test_labels, verbose=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "ODU6EpFwquWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe7RXH3N3CWU"
      },
      "source": [
        "# RUN THIS CELL TO TEST YOUR MODEL. DO NOT MODIFY CONTENTS.\n",
        "# Test model by checking how well the model generalizes using the test set.\n",
        "loss, mae, mse = model.evaluate(test_dataset, test_labels, verbose=2)\n",
        "\n",
        "print(\"Testing set Mean Abs Error: {:5.2f} expenses\".format(mae))\n",
        "\n",
        "if mae < 3500:\n",
        "  print(\"You passed the challenge. Great job!\")\n",
        "else:\n",
        "  print(\"The Mean Abs Error must be less than 3500. Keep trying.\")\n",
        "\n",
        "# Plot predictions.\n",
        "test_predictions = model.predict(test_dataset).flatten()\n",
        "\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True values (expenses)')\n",
        "plt.ylabel('Predictions (expenses)')\n",
        "lims = [0, 50000]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims,lims)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
